---
title: "Simulation Code"
output: pdf_document
date: "2024-03-06"
---

```{r message=FALSE}
library(MASS)
library(tidyverse)
library(caret)
library(class)
library(rpart)
library(randomForest)

set.seed(98)
```

# Data Generation

```{r}
# Function to generate synthetic data for a given scenario
generate_synthetic_data <- function(n=3000, imbalance_ratio, case) {
  # Calculate the number of samples for each class
  n_minority <- round(n * imbalance_ratio)
  n_majority <- n - n_minority
  
  # Set the distribution parameters based on the case
  if (case == '1a') {
    mean_X1_majority <- 0; mean_X1_minority <- 2
    lambda_X2_majority <- 1; lambda_X2_minority <- 1/2
  } else if (case == '1b') {
    mean_X1_majority <- 0; mean_X1_minority <- 2
    lambda_X2_majority <- 1/2; lambda_X2_minority <- 1
  } else if (case == '2a') {
    mean_X1_majority <- 0; mean_X1_minority <- 6
    lambda_X2_majority <- 1; lambda_X2_minority <- 1/6
  } else if (case == '2b') {
    mean_X1_majority <- 0; mean_X1_minority <- 6
    lambda_X2_majority <- 1/6; lambda_X2_minority <- 1
  }
  
  # Generate data
  X1_majority <- rnorm(n_majority, mean_X1_majority, 2)
  X1_minority <- rnorm(n_minority, mean_X1_minority, 2)
  X2_majority <- rexp(n_majority, rate=lambda_X2_majority)
  X2_minority <- rexp(n_minority, rate=lambda_X2_minority)
  
  # Combine the data
  X1 <- c(X1_majority, X1_minority)
  X2 <- c(X2_majority, X2_minority)
  Y <- c(rep(0, n_majority), rep(1, n_minority))
  
  # Shuffle the data
  data <- data.frame(X1, X2, Y)
  data <- data[sample(nrow(data)), ]
  
  return(data)
}

# Define the class imbalance ratios and cases to loop through
imbalance_ratios <- c(0.5, 0.25, 0.1, 0.05)
cases <- c('1a', '1b', '2a', '2b')

# Loop through each imbalance ratio and case, generating the datasets
for (imbalance_ratio in imbalance_ratios) {
  for (case in cases) {
    data <- generate_synthetic_data(n=3000, imbalance_ratio=imbalance_ratio, case=case)
    # You can now use 'data' for your analysis or save it for later use
    # For example, you might save each dataset to a file:
    write.csv(data, sprintf("data_%s_%s.csv", imbalance_ratio, case), row.names = FALSE)
  }
}

# This code will generate 16 datasets, one for each combination of imbalance ratio and case

```

# Fitting Models

```{r message=FALSE}
# only testing one dataset for now
data <- read_csv("data_0.5_1a.csv")
```

## Train-test split

```{r}
# get indices stratifying by class
train_idx <- createDataPartition(data$Y, p=0.7, list=FALSE)

# split the data
train_data <- data[train_idx,]
test_data <- data[-train_idx,]
```

## Logistic Regression

```{r}
# fit the model
logit <- glm(Y~X1 + X2,
             data=train_data, family="binomial")
```

## KNN

```{r}
train_scaled <- data.frame(X1_scaled = scale(train_data$X1), 
                           X2_scaled = scale(train_data$X2), 
                           Y = train_data$Y)

test_scaled <- data.frame(X1_scaled = scale(test_data$X1), 
                           X2_scaled = scale(test_data$X2), 
                           Y = test_data$Y)

cvIndex <- createFolds(factor(train_scaled$Y), 5, returnTrain=T)
train_control <- trainControl(index=cvIndex,
                              method="cv",
                              number=5)

knn_CV <- train(factor(Y)~X1_scaled+X2_scaled,
      data=train_scaled,
      method="knn",
      trControl = train_control,
      tuneGrid = data.frame(k = seq(5, 50, 5)))

final_knn <- knn3(factor(Y)~X1_scaled+X2_scaled,
                 data=train_scaled,
                 k = knn_CV$bestTune$k)
```

## Decision Trees

```{r}
# Decision Trees
cvIndex <- createFolds(factor(train_data$Y), 5, returnTrain=T)
train_control <- trainControl(index=cvIndex,
                              method="cv",
                              number=5)

validatedTree <- train(factor(Y)~X1+X2,
              data=train_data,
              method="rpart",
              trControl=train_control,
              tuneLength=10)

finalTree <- rpart(factor(Y)~X1+X2, data=train_data, control=list(cp=validatedTree$bestTune$cp))
```

## Random Forest

```{r}
cvIndex <- createFolds(factor(train_data$Y), 5, returnTrain=T)
train_control <- trainControl(index=cvIndex,
                              method="cv",
                              number=5,
                              search="grid")

forest_cv <- train(factor(Y)~X1+X2,
              data=train_data,
              method="rf",
              trControl=train_control,
              tuneGrid=expand.grid(mtry=c(1,2)),
              ntree=100)

finalForest <- randomForest(factor(Y)~X1+X2,
                            data=train_data,
                            ntree=100,
                            mtry=forest_cv$bestTune$mtry)
```


# Evaluation on Unbalanced Data

## Accuracy

```{r}
# Logistic Regression
get_logit_preds <- function(model, data) {
  yhat <- predict(model, new=data)
  phat <- exp(yhat) / (1+exp(yhat))
  preds <- round(phat)
  return (preds)
}

train_logit_preds <- get_logit_preds(logit, train_data)
mean(train_logit_preds == train_data$Y)

test_logit_preds <- get_logit_preds(logit, test_data)
mean(test_logit_preds == test_data$Y)
```

```{r}
# knn
train_knn_preds <- predict(final_knn, newdata=train_scaled, type="class")
mean(train_knn_preds == train_scaled$Y)

test_knn_preds <- predict(final_knn, newdata=test_scaled, type="class")
mean(test_knn_preds == test_scaled$Y)
```


```{r}
# Decision Trees
train_tree_preds <- predict(finalTree, newdata = train_data, type="class")
mean(train_tree_preds == train_data$Y)

test_tree_preds <- predict(finalTree, newdata = test_data, type="class")
mean(test_tree_preds == test_data$Y)
```

```{r}
# Random Forest
train_forest_preds <- predict(finalForest, newdata=train_data, type="class")
mean(train_forest_preds == train_data$Y)

test_forest_preds <- predict(finalForest, newdata = test_data, type="class")
mean(test_forest_preds == test_data$Y)
```


## Confusion Matrices

```{r}
# Logistic Regression Train
logit_cm_train <- confusionMatrix(factor(train_logit_preds), factor(train_data$Y))
logit_cm_train$table
logit_cm_train$byClass
```

```{r}
# Logistic Regression Test
logit_cm_test <- confusionMatrix(factor(test_logit_preds), factor(test_data$Y))
logit_cm_test$table
logit_cm_test$byClass
```

```{r}
#knn train
knn_cm_train <- confusionMatrix(factor(train_knn_preds), factor(train_data$Y))
knn_cm_train$table
knn_cm_train$byClass
```

```{r}
#knn test
knn_cm_test <- confusionMatrix(factor(test_knn_preds), factor(test_data$Y))
knn_cm_test$table
knn_cm_test$byClass
```


```{r}
# Decision Tree Train
tree_cm_train <- confusionMatrix(factor(train_tree_preds), factor(train_data$Y))
tree_cm_train$table
tree_cm_train$byClass
```

```{r}
# Decision Tree Test
tree_cm_test <- confusionMatrix(factor(test_tree_preds), factor(test_data$Y))
tree_cm_test$table
tree_cm_test$byClass
```

```{r}
# Random forest train
forest_cm_train <- confusionMatrix(factor(train_forest_preds), factor(train_data$Y))
forest_cm_train$table
forest_cm_train$byClass
```

```{r}
#random forest test
forest_cm_test <- confusionMatrix(factor(test_forest_preds), factor(test_data$Y))
forest_cm_test$table
forest_cm_test$byClass
```

