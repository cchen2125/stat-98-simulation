---
title: "Simulation Code"
output: pdf_document
date: "2024-03-06"
---

```{r message=FALSE}
library(MASS)
library(tidyverse)
library(caret)
library(class)
library(rpart)
library(randomForest)

set.seed(0)
```

# Mini simulation to test the p and $\beta_0$ relationship formula

```{r}
# p and beta 0 relationship Monte Carlo test
n_test <- 10^6
desired_p <- c(0.05, 0.25, 0.5, 0.75, 0.95)

sigmoid <- function(x) {
  return (1 / (1+exp(-x)))
}

get_beta0 <- function(beta1, beta2) {
  
  # grid of beta 0 to try
  beta0_vec <- seq(-10, 10, 0.1) 
  
  # placeholder p vector
  ps <- rep(NA, length(beta0_vec))
  
  # get empircal proportion for each beta0
  for (i in 1:length(beta0_vec)) {
    X1 <- rnorm(n, 0, 2)
    X2 <- rexp(n, 0.5) - 2
    
    probs <- sigmoid(beta0_vec[i] + beta1 * X1 + beta2 * X2)
    
    Y <- rbinom(n, 1, probs)
    
    ps[i] <- mean(Y)
  }
  
  final_beta0 <- rep(NA, length(desired_p))
  
  # find best beta0 for each desired p
  for (i in 1:length(desired_p)) {
    final_beta0[i] <- beta0_vec[which.min(abs(ps - desired_p[i]))]
  }
  
  return (final_beta0)
}

case1_beta0s <- get_beta0(0.5, 0.5)
case2_beta0s <- get_beta0(2, 2)
case3_beta0s <- get_beta0(2, 0)
case4_beta0s <- get_beta0(0, 2)

case1_beta0s
case2_beta0s
case3_beta0s
case4_beta0s
```


# Data Generation

```{r}
generate_data <- function(n, beta_0, beta_1, beta_2) {
  X1 <- rnorm(n, mean = 0, sd = 2)
  X2 <- rexp(n, rate = 0.5) - 2
  linear_combination <- beta_0 + beta_1 * X1 + beta_2 * X2
  prob <- exp(linear_combination) / (1 + exp(linear_combination))
  Y <- rbinom(n, size = 1, prob = prob)
  data <- data.frame(Y, X1, X2)
  return(data)
}
```

```{r eval=FALSE}
n_iter <- 1000
h <- 2000

for (iter in 1:1000) {
  for (i in 1:length(desired_p)) {
    data <- generate_data(n, case1_beta0s[i], 0.5, 0.5)
    write.csv(data, file.path("data/", sprintf("data_case%s_p%s_iter%s.csv", 1, desired_p[i], iter)), 
              row.names = FALSE)
  }
  
  for (i in 1:length(desired_p)) {
    data <- generate_data(n, case2_beta0s[i], 2, 2)
    write.csv(data, file.path("data/", sprintf("data_case%s_p%s_iter%s.csv", 2, desired_p[i], iter)), 
              row.names = FALSE)
  }
  
  for (i in 1:length(desired_p)) {
    data <- generate_data(n, case3_beta0s[i], 2, 0)
    write.csv(data, file.path("data/", sprintf("data_case%s_p%s_iter%s.csv", 3, desired_p[i], iter)),
              row.names = FALSE)
  }
  
  for (i in 1:length(desired_p)) {
    data <- generate_data(n, case4_beta0s[i], 0, 2)
    write.csv(data, file.path("data/", sprintf("data_case%s_p%s_iter%s.csv", 4, desired_p[i], iter)),
              row.names = FALSE)
  } 
}
```


# Fitting Models

## Train-test split

```{r}
# get indices stratifying by class
train_idx <- createDataPartition(data$Y, p=0.7, list=FALSE)

# split the data
train_data <- data[train_idx,]
test_data <- data[-train_idx,]
```

## Logistic Regression

```{r}
# fit the model
logit <- glm(Y~X1 + X2,
             data=train_data, family="binomial")
```

## KNN

```{r}
train_scaled <- data.frame(X1_scaled = scale(train_data$X1), 
                           X2_scaled = scale(train_data$X2), 
                           Y = train_data$Y)

test_scaled <- data.frame(X1_scaled = scale(test_data$X1), 
                           X2_scaled = scale(test_data$X2), 
                           Y = test_data$Y)

cvIndex <- createFolds(factor(train_scaled$Y), 5, returnTrain=T)
train_control <- trainControl(index=cvIndex,
                              method="cv",
                              number=5)

knn_CV <- train(factor(Y)~X1_scaled+X2_scaled,
      data=train_scaled,
      method="knn",
      trControl = train_control,
      tuneGrid = data.frame(k = seq(5, 50, 5)))

final_knn <- knn3(factor(Y)~X1_scaled+X2_scaled,
                 data=train_scaled,
                 k = knn_CV$bestTune$k)
```

## Decision Trees

```{r}
# Decision Trees
cvIndex <- createFolds(factor(train_data$Y), 5, returnTrain=T)
train_control <- trainControl(index=cvIndex,
                              method="cv",
                              number=5)

validatedTree <- train(factor(Y)~X1+X2,
              data=train_data,
              method="rpart",
              trControl=train_control,
              tuneLength=10)

finalTree <- rpart(factor(Y)~X1+X2, data=train_data, control=list(cp=validatedTree$bestTune$cp))
```

## Random Forest

```{r}
cvIndex <- createFolds(factor(train_data$Y), 5, returnTrain=T)
train_control <- trainControl(index=cvIndex,
                              method="cv",
                              number=5,
                              search="grid")

forest_cv <- train(factor(Y)~X1+X2,
              data=train_data,
              method="rf",
              trControl=train_control,
              tuneGrid=expand.grid(mtry=c(1,2)),
              ntree=100)

finalForest <- randomForest(factor(Y)~X1+X2,
                            data=train_data,
                            ntree=100,
                            mtry=forest_cv$bestTune$mtry)
```


# Evaluation on Unbalanced Data

## Accuracy

```{r}
# Logistic Regression
get_logit_preds <- function(model, data) {
  yhat <- predict(model, new=data)
  phat <- exp(yhat) / (1+exp(yhat))
  preds <- round(phat)
  return (preds)
}

train_logit_preds <- get_logit_preds(logit, train_data)
mean(train_logit_preds == train_data$Y)

test_logit_preds <- get_logit_preds(logit, test_data)
mean(test_logit_preds == test_data$Y)
```

```{r}
# knn
train_knn_preds <- predict(final_knn, newdata=train_scaled, type="class")
mean(train_knn_preds == train_scaled$Y)

test_knn_preds <- predict(final_knn, newdata=test_scaled, type="class")
mean(test_knn_preds == test_scaled$Y)
```


```{r}
# Decision Trees
train_tree_preds <- predict(finalTree, newdata = train_data, type="class")
mean(train_tree_preds == train_data$Y)

test_tree_preds <- predict(finalTree, newdata = test_data, type="class")
mean(test_tree_preds == test_data$Y)
```

```{r}
# Random Forest
train_forest_preds <- predict(finalForest, newdata=train_data, type="class")
mean(train_forest_preds == train_data$Y)

test_forest_preds <- predict(finalForest, newdata = test_data, type="class")
mean(test_forest_preds == test_data$Y)
```


## Confusion Matrices

```{r}
# Logistic Regression Train
logit_cm_train <- confusionMatrix(factor(train_logit_preds), factor(train_data$Y))
logit_cm_train$table
logit_cm_train$byClass
```

```{r}
# Logistic Regression Test
logit_cm_test <- confusionMatrix(factor(test_logit_preds), factor(test_data$Y))
logit_cm_test$table
logit_cm_test$byClass
```

```{r}
#knn train
knn_cm_train <- confusionMatrix(factor(train_knn_preds), factor(train_data$Y))
knn_cm_train$table
knn_cm_train$byClass
```

```{r}
#knn test
knn_cm_test <- confusionMatrix(factor(test_knn_preds), factor(test_data$Y))
knn_cm_test$table
knn_cm_test$byClass
```


```{r}
# Decision Tree Train
tree_cm_train <- confusionMatrix(factor(train_tree_preds), factor(train_data$Y))
tree_cm_train$table
tree_cm_train$byClass
```

```{r}
# Decision Tree Test
tree_cm_test <- confusionMatrix(factor(test_tree_preds), factor(test_data$Y))
tree_cm_test$table
tree_cm_test$byClass
```

```{r}
# Random forest train
forest_cm_train <- confusionMatrix(factor(train_forest_preds), factor(train_data$Y))
forest_cm_train$table
forest_cm_train$byClass
```

```{r}
#random forest test
forest_cm_test <- confusionMatrix(factor(test_forest_preds), factor(test_data$Y))
forest_cm_test$table
forest_cm_test$byClass
```

